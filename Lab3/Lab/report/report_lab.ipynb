{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> 3η Εργαστηριακή Άσκηση</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Σύνδεση με προπαρασκευή και διόρθωση σφαλμάτων</h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η δομή του project είναι παρόμοια με αυτή της προπαρασκευής με την διαφορά ότι τώρα θα βασιστούμε μόνο στο __Semeval 2017 Task4-A dataset__ και θα έχουμε πολλά διαφορετικά μοντέλα. Έτσι, δημιουργήθηκε ο φάκελος __models/__ όπου μέσα θα περιέχει όλα τα μοντέλα που θα χρησιμοποιήσουμε για την εκπαίδευση του μοντέλου (επίσης στον κώδικα διέγραψα ότι αφορούσε την προπαρασκευή όπως κάποιες εκτυπώσεις ή το loss function για το MR dataset). Τα embeddings θα παραμείνουν τα __twitter glove embeddings__ που βρίσκονται στον φάκελο __embeddings/__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Προτού συνεχίσουμε όμως στα βήματα του εργαστηρίου και την κατασκευή των μοντέλων πρέπει να επισημανθούν ορισμένες αλλαγές που έγιναν στον κώδικα της προπαρασκευής μιας και αυτός θα χρησιμοποιηθεί και ως βάση για την συνέχεια. Συγκεκριμένα, είχα παρατηρήσει ότι το test loss δεν ήταν καθόλου καλό και έκανε συνεχώς \"ταλαντώσεις\" οπότε προσπάθησα να βελτιώσω και άλλο την προπεξεργασία των δεδομένων και τον ορισμό των υπερπαραμέτρων. Τα σημαντικά λάθη που έγιναν και διορθώθηκαν αναφέρονται παρακάτω:\n",
    "\n",
    "- Στο tokenization __δεν μετέτρεπα τις λέξεις σε lowercase__ με αποτέλεσμα χωρίς lower case να έχω 369007 στο train και 83017 στο test oov words σε train και test set αντίστοιχα ενώ μετά το lowercase να έχω 126054 και 38865 oov words. Υπολογίζοντας ότι ο συνολικός αριθμός λέξεων στο train και στο test set είναι 1206783 και 233218 αντίστοιχα, __έχουμε μείωση περίπου 20% στο train και 19% στο test set των oov words__.\n",
    "\n",
    "- Το tokenization που έκανα με τον Twitter tokenizer του nltk δεν είχε πολύ καλά αποτελέσματα για το dataset του Semeval2017A οπότε __χρησιμοποιήσα τον SocialTokenizer του ekphrasis__ με αποτέλεσμα οι oov words να __μειωθούν κατά 2% στα δυο set__ (έχουμε 99050 ή 8% τελικά στο train set και 33664 ή 14% στο test set).\n",
    "\n",
    "- Μία ακόμα αλλαγή που έγινε είναι στην επιλογή του σταθερού μήκους που πρέπει να έχουν οι προτάσεις, όπου οι μικρότερες προτάσεις συμπληρώνονται με μηδενικά (zero padding) και οι μεγαλύτερες κόβονται. Η επιλογή γινόταν μέσω του __bestLength.py__ στο οποίο εμφανίζα ένα scatterplot με τα μεγέθη για κάθε πρόταση και επέλεγα το 40 ώστε να κοπούν μόνο τα outliers. Αυτό που δεν φαινόταν όμως είναι ότι οι περισσότερες προτάσεις έχουν μικρότερο μήκος απλά το scatter plot ήταν πολύ πυκνή αναπαράσταση για να το δέιξει. Έτσι, τροποποίηθηκε το bestLength.py ώστε να εμφανίζει ένα ιστόγραμμα με τα μεγέθη, την μέση τιμή και την τυπική απόκλιση για κάθε ένα από τα δύο set μας. Όταν το τρέχουμε βλέπουμε τα παρακάτω:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train set: 49570\n",
      "Mean value of train set: 24.345027234214243\n",
      "Variance of train set is: 6.685360783381769\n",
      "Size of test set: 12284\n",
      "Mean value of test set: 18.985509605991535\n",
      "Variance of test set is: 6.903150432448993\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ../bestLength.py Semeval2017A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρούμε, λοιπόν, ότι το 30 είναι μία καλή επιλόγη για το μέγεθος και όχι το 40 που είχα διότι οι περισσότερες προτάσεις είναι μέχρι εκεί οπότε και δεν θα χάσουμε πληροφορία και παράλληλα δεν θα έχουμε πολλά μηδενικά στις μικρότερες προτάσεις."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Τέλος, άλλαξα και το hidden size του BaselineDNN που χρησιμοποιούσαμε στην προπαρασκευή διότι ήταν μεγάλο (128) και το μοντέλο έκανε πολύ γρήγορα overfit (γι'αυτό και οι ταλαντώσεις). Μετά από ορισμένες δοκιμές το όρισα ίσο με 32. Ακόμη, οι εποχές μειώθηκαν σε 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h2> Εισαγωγή </h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στην προπαρασκευή, σχεδιάσαμε απλές αρχιτεκτονικές νευρωνικών δικτύων για κατηγοριοποίηση κειμένων, χρησιμοποιώντας προ-εκπαιδευμένες διανυσματικές αναπαραστάσεις λέξεων (pretrained word embeddings). Σκοπός αυτής της εργαστηριακής άσκησης είναι να εμβαθύνουμε χρησιμοποιώντας __Ανατροφοδοτούμενα Νευρωνικά Δίκτυα (Recurrent Neural Networks), τεχνικές Μεταφοράς Γνώσης (Transfer Learning) και μηχανισμούς Προσοχής (Attention mechanisms).__ Στην προπαρασκευή του εργαστηρίου, η αρχιτεκτονική η οποία μας έχει ζητηθεί να φτιάξουμε είναι η εξής:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"photos/prolab-equations.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "όπου r η διανυσματική αναπαράσταση ενός κειμένου (feature vector). Στόχος είναι να δημιουργήσουμε καλύτερες αναπαραστάσεις χρησιμοποιώντας τις προαναφερθείσες τεχνικές."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2> Επιλογή συνόλου δεδομένων και pretrained word embeddings </h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Το σύνολο δεδομένων που θα χρησιμοποιήσουμε είναι το __Semeval 2017 Task4-A__ 3 [Rosenthal et al., 2017]. To dataset αυτό περιέχει tweets τα οποία είναι κατηγοριοποιημένα σε 3 κλάσεις (positive, negative, neutral) με 49570 παραδείγματα εκπαίδευσης και 12284 παραδείγματα αξιολόγησης.\n",
    "- Τα word embeddings που θα χρησιμοποιήσουμε είναι τα __Glove twitter embeddings__ τα οποία είναι διαθέσιμα [εδώ](https://nlp.stanford.edu/projects/glove/). Συγκεκριμένα όλα τα αποτελέσματα που θα παρουσιαστούν έχουν προκύψει στα embeddings 50 διαστάσεων τα οποία λόγω του μεγέθους του είναι πολύ αποδοτικά (επηρεάζοντας βέβαια αρνητικά την επίδοση του μοντέλου)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h2> Ερωτήματα </h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ερώτημα 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"photos/question1-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Πρέπει, λοιπόν, να τροποποιήσουμε την μέθοδο __forward__ στο BaselineDNN() όπου στην προπαρασκευή η αναπαράσταση κάθε πρότασης υπολογιζόταν ως το mean pooling των word embeddings. Τώρα, μαζί με το mean pooling θα υπολογίζουμε και το max pooling και θα συνενώνουμε τις δύο αυτές αναπαραστάσις για να προκύψει η τελική αναπαράσταση της πρότασης. Όπως και στην προπαρασκευή, για τον σταδιακό έλεγχο του μοντέλου μας υλοποιούμε την όλη διαδικασία για ένα ενδεικτικό mini-batch και βλέπουμε ότι η αναπαράσταση είναι η αναμενόμενη. Επίσης, πρέπει να αλλάξουμε και τον ορισμό του πρώτου Linear Layer που κάνει προβολή των αρχικών features στο hidden size αφού τώρα παίρνει ως είσοδο το διπλάσιο μέγεθος από πριν."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS\n",
      "Input batch shape is:  torch.Size([2, 6])\n",
      "Input batch is: \n",
      "tensor([[3, 0, 0, 1, 2, 1],\n",
      "        [0, 1, 2, 3, 1, 0]])\n",
      "Input lengths array is:  [5 3]\n",
      "\n",
      "STEP 1\n",
      "Output of embedding layer has shape:  torch.Size([2, 6, 5])\n",
      "Output of embedding layer is: \n",
      "tensor([[[4., 3., 2., 1., 0.],\n",
      "         [0., 1., 2., 3., 4.],\n",
      "         [0., 1., 2., 3., 4.],\n",
      "         [0., 2., 4., 6., 8.],\n",
      "         [1., 3., 5., 7., 9.],\n",
      "         [0., 2., 4., 6., 8.]],\n",
      "\n",
      "        [[0., 1., 2., 3., 4.],\n",
      "         [0., 2., 4., 6., 8.],\n",
      "         [1., 3., 5., 7., 9.],\n",
      "         [4., 3., 2., 1., 0.],\n",
      "         [0., 2., 4., 6., 8.],\n",
      "         [0., 1., 2., 3., 4.]]])\n",
      "\n",
      "STEP 2\n",
      "Mean pooling representation of sentences has shape:  torch.Size([2, 5])\n",
      "Mean pooling representation of sentences: \n",
      "tensor([[ 1.0000,  2.4000,  3.8000,  5.2000,  6.6000],\n",
      "        [ 1.6667,  4.0000,  6.3333,  8.6667, 11.0000]])\n",
      "\n",
      "STEP 3\n",
      "Max pooling representation of sentences has shape:  torch.Size([2, 5])\n",
      "Max pooling representation of sentences: \n",
      "tensor([[4., 3., 5., 7., 9.],\n",
      "        [4., 3., 5., 7., 9.]])\n",
      "\n",
      "STEP 4\n",
      "Final representation of sentences has shape:  torch.Size([2, 10])\n",
      "Final representation of sentences: \n",
      "tensor([[ 1.0000,  2.4000,  3.8000,  5.2000,  6.6000,  4.0000,  3.0000,  5.0000,\n",
      "          7.0000,  9.0000],\n",
      "        [ 1.6667,  4.0000,  6.3333,  8.6667, 11.0000,  4.0000,  3.0000,  5.0000,\n",
      "          7.0000,  9.0000]])\n"
     ]
    }
   ],
   "source": [
    "# -------------------- #\n",
    "# Forward pass testing #\n",
    "# -------------------- #\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Lengths definitions\n",
    "max_length = 6\n",
    "n_embeddings = 4\n",
    "batch_size = 2\n",
    "embeddings_size = 5\n",
    "hidden_size = 8\n",
    "\n",
    "# Create embeddings array\n",
    "embeddings = np.array([[0, 1, 2, 3, 4],[0, 2, 4, 6, 8], [1, 3, 5, 7, 9], [4, 3, 2, 1, 0]])\n",
    "# Define a embedding layer\n",
    "embed = nn.Embedding(num_embeddings = n_embeddings, embedding_dim = embeddings_size)\n",
    "# Initialize the weights of the embedding layer with embeddings array.\n",
    "embed.load_state_dict({'weight': torch.from_numpy(embeddings)})\n",
    "# Make embedding layer non-trainable\n",
    "embed.weight.requires_grad = False\n",
    "\n",
    "# Create a mini batch with some data\n",
    "x = torch.from_numpy(np.array([[3, 0, 0, 1, 2, 1], [0, 1, 2, 3, 1, 0]])).long()\n",
    "# Create the lengths array\n",
    "lengths = np.array([5, 3])\n",
    "print(\"INPUTS\")\n",
    "print(\"Input batch shape is: \", x.shape)\n",
    "print(\"Input batch is: \")\n",
    "print(x)\n",
    "print(\"Input lengths array is: \", lengths)\n",
    "print()\n",
    "\n",
    "# 1. Pass it through the embedding layer.\n",
    "embeddings = embed(x)\n",
    "print(\"STEP 1\")\n",
    "print(\"Output of embedding layer has shape: \", embeddings.shape)\n",
    "print(\"Output of embedding layer is: \")\n",
    "print(embeddings)\n",
    "print()\n",
    "\n",
    "# 2. Create a representation for each sentence by computing the mean pooling.\n",
    "representations_mean = torch.zeros([batch_size, embeddings_size])\n",
    "for i in range(batch_size):\n",
    "    representations_mean[i] = torch.sum(embeddings[i], dim=0) / lengths[i]\n",
    "print(\"STEP 2\")\n",
    "print(\"Mean pooling representation of sentences has shape: \", representations_mean.shape)\n",
    "print(\"Mean pooling representation of sentences: \")\n",
    "print(representations_mean)\n",
    "print()\n",
    "\n",
    "# 3. Create a representation for each sentence by computing the max pooling.\n",
    "representations_max = torch.zeros([batch_size, embeddings_size])\n",
    "for i in range(batch_size):\n",
    "    representations_max[i],_ = torch.max(embeddings[i], dim=0)\n",
    "print(\"STEP 3\")\n",
    "print(\"Max pooling representation of sentences has shape: \", representations_max.shape)\n",
    "print(\"Max pooling representation of sentences: \")\n",
    "print(representations_max)\n",
    "print()\n",
    "\n",
    "# 3. Create the final representation for each sentence by concatenating mean and max pooling.\n",
    "representations = torch.cat((representations_mean, representations_max), 1)\n",
    "print(\"STEP 4\")\n",
    "print(\"Final representation of sentences has shape: \", representations.shape)\n",
    "print(\"Final representation of sentences: \")\n",
    "print(representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Και τα αποτελέσματα με αυτό το νέο BaselineDNN σε 30 εποχές με batch size ίσο με 120 και hidden layer size ίσο με 32 είναι τα παρακάτω:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"photos/MeanMax_output.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   <tr>\n",
    "    <td> <img src=\"photos/MeanMax_train.png\" alt=\"Drawing\" style=\"width: 650px;\"/> </td>\n",
    "    <td> <img src=\"photos/MeanMax_test.png\" alt=\"Drawing\" style=\"width: 650px;\"/> </td>\n",
    "    </tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"photos/question1-2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στο mean pooling, όπου η αναπαράσταση μιας πρότασης προκύπτει ως ο μέσος όρος των word embeddings, λέμε ότι ουσιαστικά (αν σκεφτούμε ότι κάθε διάσταση των embeddings εκφράζει μία έννοια) η σημασία της πρότασης εξαρτάται από το πόσο πολύ σχετίζονται με κάθε μία από αυτές τις έννοιες οι λέξεις που την απαρτίζουν. Όταν παίρνουμε τον μέσο όρο, είναι σαν να λέμε ότι η πρόταση εκφράζει την έννοια x αν οι περισσότερες λέξεις της εκφράζουν την έννοια x. Το πρόβλημα εδώ όμως είναι ότι υπάρχουν λέξεις με πολύ \"ισχυρή\" σημασία σε κάποια διάσταση οι οποίες μπορεί να καθορίζουν και τη σημασία ολόκληρης της πρότασης χωρίς οι υπόλοιπες λέξεις να είναι κόντα στην έννοια αυτή (πχ το 'δεν' για κάποια πιθανή διάσταση που εκφράζει άρνηση επηρεάζει όλη την πρόταση). Για τον λόγο αυτό παίρνουμε και το max pooling των word embeddings έτσι ώστε οι λέξεις με την μεγαλύτερη τιμή σε κάθε έννοια-διάσταση να παίζουν σημαντικό ρόλο στην τελική σημασιολογική αναπαράσταση της πρότασης."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ερώτημα 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"photos/question2-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Πρέπει, λοιπόν, να φτιάξουμε ένα νέο μοντέλο __LSTMNet__ το οποίο θα το αποθηκεύσουμε στον φάκελο __models/__ μαζί με όλα μοντέλα μας. Το μοντέλο μας θα έχει αρχικά ένα embedding layer για να μετατρέπει το input σε word embeddings, στη συνέχεια το LSTM και τέλος έναν γραμμικό μετασχηματισμό για να προβάλλουμε την τελευταία έξοδο του LSTM στον αριθμό των κλάσεων που έχουμε (θα μπορούσαμε να έχουμε και άλλο layer αλλά αποφεύγεται λόγω πιθανού overfit). Όπως αναφέρεται, επίσης, πρέπει να χρησιμοποιήσουμε για κάθε πρόταση το πραγματικά τελευταίο timestep, εξαιρώντας τα zero-padded timesteps. Παρακάτω βλέπουμε μία δοκιμή του μοντέλου σε δύο προτάσεις για να ελέγξουμε ότι οι διαστάσεις κάθε φορά είναι οι σωστές. Ο κώδικας βρίσκεται στο αρχέιο model/LSTMNet.py όπου υπάρχουν και αναλυτικά σχόλια."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS\n",
      "Input batch shape is:  torch.Size([2, 6])\n",
      "Input batch is: \n",
      "tensor([[3, 0, 0, 1, 2, 1],\n",
      "        [0, 1, 2, 3, 1, 0]])\n",
      "Input lengths array is:  tensor([5, 3])\n",
      "\n",
      "STEP 1\n",
      "Output of embedding layer has shape:  torch.Size([2, 6, 5])\n",
      "Output of embedding layer is: \n",
      "tensor([[[4., 3., 2., 1., 0.],\n",
      "         [0., 1., 2., 3., 4.],\n",
      "         [0., 1., 2., 3., 4.],\n",
      "         [0., 2., 4., 6., 8.],\n",
      "         [1., 3., 5., 7., 9.],\n",
      "         [0., 2., 4., 6., 8.]],\n",
      "\n",
      "        [[0., 1., 2., 3., 4.],\n",
      "         [0., 2., 4., 6., 8.],\n",
      "         [1., 3., 5., 7., 9.],\n",
      "         [4., 3., 2., 1., 0.],\n",
      "         [0., 2., 4., 6., 8.],\n",
      "         [0., 1., 2., 3., 4.]]])\n",
      "\n",
      "STEP 2\n",
      "Output of LSTM has shape:  torch.Size([2, 6, 4])\n",
      "Output of LSTM is: \n",
      "tensor([[[-2.1566e-01,  7.5638e-02,  1.5589e-02, -4.0411e-02],\n",
      "         [-4.5627e-01,  9.4205e-03,  2.6802e-02,  3.6949e-01],\n",
      "         [-4.8128e-01, -1.4231e-02,  5.1189e-02,  5.5067e-01],\n",
      "         [-7.2285e-01, -1.2559e-04,  4.0303e-02,  7.6502e-01],\n",
      "         [-7.8970e-01,  1.2756e-02,  4.0474e-02,  7.8599e-01],\n",
      "         [-7.1419e-01,  2.5810e-02,  4.7232e-02,  7.9830e-01]],\n",
      "\n",
      "        [[-4.2866e-01, -3.7758e-02,  1.2345e-02,  3.4154e-01],\n",
      "         [-7.0930e-01, -4.5477e-03,  1.2425e-02,  7.4781e-01],\n",
      "         [-7.8232e-01,  1.0919e-02,  1.3486e-02,  7.8410e-01],\n",
      "         [-3.8387e-01,  2.0364e-01,  9.0931e-02,  2.1853e-01],\n",
      "         [-7.1223e-01,  2.4425e-02,  4.9926e-02,  7.3911e-01],\n",
      "         [-4.6635e-01,  9.3041e-02,  1.0014e-01,  6.8818e-01]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "STEP 3\n",
      "The representation of the sentence has shape:  torch.Size([2, 4])\n",
      "The representation of the sentence is: \n",
      "tensor([[-0.7897,  0.0128,  0.0405,  0.7860],\n",
      "        [-0.7823,  0.0109,  0.0135,  0.7841]], grad_fn=<CopySlices>)\n",
      "\n",
      "STEP 4\n",
      "Logits have shape:  torch.Size([2, 3])\n",
      "Logits are: \n",
      "tensor([[-0.5603, -0.4643,  0.1445],\n",
      "        [-0.5583, -0.4546,  0.1546]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# ------------------ #\n",
    "# Lstm layer testing #\n",
    "# ------------------ #\n",
    "# Lengths definitions\n",
    "output_size = 3\n",
    "max_length = 6\n",
    "batch_size = 2\n",
    "embeddings_size = 5\n",
    "hidden_size = 4\n",
    "# Input: x is a mini-batch of sentences with length = max_length and 'lengths' array.\n",
    "# Create tensor x\n",
    "x = torch.from_numpy(np.array([[3, 0, 0, 1, 2, 1], [0, 1, 2, 3, 1, 0]])).long()\n",
    "# Create the lengths array\n",
    "lengths = torch.tensor([5, 3])\n",
    "print(\"INPUTS\")\n",
    "print(\"Input batch shape is: \", x.shape)\n",
    "print(\"Input batch is: \")\n",
    "print(x)\n",
    "print(\"Input lengths array is: \", lengths)\n",
    "print()\n",
    "\n",
    "# Pass through the embedding layer which converts x in a 3-D tensor where:\n",
    "# Axis 0 represents a sentence in the mini batch.\n",
    "# Axis 1 represents each word in the sentence.\n",
    "# Axis 2 represents the embedding of the word.\n",
    "embeddings = embed(x)  \n",
    "print(\"STEP 1\")\n",
    "print(\"Output of embedding layer has shape: \", embeddings.shape)\n",
    "print(\"Output of embedding layer is: \")\n",
    "print(embeddings)\n",
    "print()\n",
    "\n",
    "# Define lstm layer which takes the embedding of the words of a sentence.\n",
    "hidden_size = 4\n",
    "num_layers = 1\n",
    "# Set initial hidden and cell states\n",
    "h0 = torch.zeros(num_layers, x.size(0), hidden_size)\n",
    "c0 = torch.zeros(num_layers, x.size(0), hidden_size)\n",
    "lstm = nn.LSTM(embeddings_size, hidden_size, num_layers, batch_first=True)\n",
    "# Pass throught the lstm layer and get the output which contains\n",
    "# the output features (h_t) from the last layer of the LSTM.\n",
    "# Representations have shape (batch_size, max_length, hidden_size)\n",
    "lstm_out, _ = lstm(embeddings, (h0, c0))\n",
    "print(\"STEP 2\")\n",
    "print(\"Output of LSTM has shape: \", lstm_out.shape)\n",
    "print(\"Output of LSTM is: \")\n",
    "print(lstm_out)\n",
    "print()\n",
    "\n",
    "# Get the last output of the LSTM hn as representation.\n",
    "# It is important here to take the last output without zero padding.\n",
    "representations = torch.zeros(batch_size, hidden_size).float()\n",
    "for i in range(batch_size):\n",
    "    representations[i] = lstm_out[i, lengths[i]-1, :]\n",
    "print(\"STEP 3\")\n",
    "print(\"The representation of the sentence has shape: \", representations.shape)\n",
    "print(\"The representation of the sentence is: \")\n",
    "print(representations)\n",
    "print()\n",
    "\n",
    "# Define linear layer to project to output shape.\n",
    "linear = nn.Linear(hidden_size, output_size)\n",
    "# Pass the representation of each sentence through linear layer.\n",
    "logits = linear(representations)\n",
    "print(\"STEP 4\")\n",
    "print(\"Logits have shape: \", logits.shape)\n",
    "print(\"Logits are: \")\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tα αποτελέσματα με το LSTMNet σε 30 εποχές με batch size ίσο με 120, ένα layer στο lstm και hidden layer size ίσο με 16 είναι τα παρακάτω:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"photos/LSTM_output.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   <tr>\n",
    "    <td> <img src=\"photos/LSTM_train.png\" alt=\"Drawing\" style=\"width: 650px;\"/> </td>\n",
    "    <td> <img src=\"photos/LSTM_test.png\" alt=\"Drawing\" style=\"width: 650px;\"/> </td>\n",
    "    </tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"photos/question2-2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η παραπάνω διαδικασία θα γίνει στο __models/LSTMpool.py__ όπου ουσιαστικά θα κάνουμε ακριβώς την ίδια διαδικασία με το models/LSTMNet.py που κάναμε παραπάνω απλά στο τέλος αντί να πάρουμε ως αναπαράσταση του κειμένου την τελευταία έξοδο (λαμβάνοντας υπόψιν ότι έχουμε zero padding) θα πάρουμε την ζητούμενη συνένωση. Άρα, οι αλλαγές που θα γίνουν είναι ότι στην αρχικοποίηση του μοντέλου θα δηλώσουμε ότι το τελευταίο linear layer που παίρνει την αναπαράσταση των κειμένων θα παίρνει αναπαραστάσεις μεγέθους 3*hidden_size και στο forward του μοντέλου θα δίνουμε στον linear layer την παραπάνω αναπαράσταση με αντίστοιχο τρόπο όπως στο πρώτο ερώτημα. Παρακάτω βλέπουμε την συνέχεια της παραπάνω διαδικασίας με τις αλλαγές."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the previous step\n",
      "Output of LSTM has shape:  torch.Size([2, 6, 4])\n",
      "Output of LSTM is:  tensor([[[-2.1566e-01,  7.5638e-02,  1.5589e-02, -4.0411e-02],\n",
      "         [-4.5627e-01,  9.4205e-03,  2.6802e-02,  3.6949e-01],\n",
      "         [-4.8128e-01, -1.4231e-02,  5.1189e-02,  5.5067e-01],\n",
      "         [-7.2285e-01, -1.2559e-04,  4.0303e-02,  7.6502e-01],\n",
      "         [-7.8970e-01,  1.2756e-02,  4.0474e-02,  7.8599e-01],\n",
      "         [-7.1419e-01,  2.5810e-02,  4.7232e-02,  7.9830e-01]],\n",
      "\n",
      "        [[-4.2866e-01, -3.7758e-02,  1.2345e-02,  3.4154e-01],\n",
      "         [-7.0930e-01, -4.5477e-03,  1.2425e-02,  7.4781e-01],\n",
      "         [-7.8232e-01,  1.0919e-02,  1.3486e-02,  7.8410e-01],\n",
      "         [-3.8387e-01,  2.0364e-01,  9.0931e-02,  2.1853e-01],\n",
      "         [-7.1223e-01,  2.4425e-02,  4.9926e-02,  7.3911e-01],\n",
      "         [-4.6635e-01,  9.3041e-02,  1.0014e-01,  6.8818e-01]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "Last output with length 5 and 3\n",
      "The last output has shape: \n",
      "torch.Size([2, 4])\n",
      "The  last output is: \n",
      "tensor([[-0.7897,  0.0128,  0.0405,  0.7860],\n",
      "        [-0.7823,  0.0109,  0.0135,  0.7841]], grad_fn=<CopySlices>)\n",
      "\n",
      "The mean output has shape: \n",
      "torch.Size([2, 4])\n",
      "The mean output is: \n",
      "tensor([[-2.6658,  0.0835,  0.1744,  2.4308],\n",
      "        [-1.9203, -0.0314,  0.0383,  1.8735]], grad_fn=<CopySlices>)\n",
      "\n",
      "The max output has shape:  torch.Size([2, 4])\n",
      "The max output is: \n",
      "tensor([[-0.2157,  0.0756,  0.0512,  0.7860],\n",
      "        [-0.4287,  0.0109,  0.0135,  0.7841]], grad_fn=<CopySlices>)\n",
      "\n",
      "The representation has shape:  torch.Size([2, 12])\n",
      "The representation is: \n",
      "tensor([[-0.7897,  0.0128,  0.0405,  0.7860, -2.6658,  0.0835,  0.1744,  2.4308,\n",
      "         -0.2157,  0.0756,  0.0512,  0.7860],\n",
      "        [-0.7823,  0.0109,  0.0135,  0.7841, -1.9203, -0.0314,  0.0383,  1.8735,\n",
      "         -0.4287,  0.0109,  0.0135,  0.7841]], grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(\"From the previous step\")\n",
    "print(\"Output of LSTM has shape: \", lstm_out.shape)\n",
    "print(\"Output of LSTM is: \", lstm_out)\n",
    "# ------------------- #\n",
    "# Last output of LSTM #\n",
    "# ------------------- #\n",
    "# Get the last output of the LSTM hn as representation.\n",
    "# It is important here to take the last output without zero padding.\n",
    "last = torch.zeros(batch_size, hidden_size).float()\n",
    "for i in range(batch_size):\n",
    "    last[i] = lstm_out[i, lengths[i]-1, :]\n",
    "print(\"Last output with length 5 and 3\")\n",
    "print(\"The last output has shape: \")\n",
    "print(last.shape)\n",
    "print(\"The  last output is: \")\n",
    "print(last)\n",
    "print()\n",
    "\n",
    "# ----------------------------- #\n",
    "# Mean value of outputs of LSTM #\n",
    "# ----------------------------- #\n",
    "mean_pool = torch.zeros([batch_size, hidden_size]).float()\n",
    "for i in range(batch_size):\n",
    "    mean_pool[i] = torch.sum(lstm_out[i, :lengths[i], :], dim=0) \n",
    "print(\"The mean output has shape: \")\n",
    "print(mean_pool.shape)\n",
    "print(\"The mean output is: \")\n",
    "print(mean_pool)\n",
    "print()\n",
    "\n",
    "# ---------------------------- #\n",
    "# Max value of outputs of LSTM #\n",
    "# ---------------------------- #\n",
    "max_pool = torch.zeros([batch_size, hidden_size]).float()\n",
    "for i in range(batch_size):\n",
    "    max_pool[i],_ = torch.max(lstm_out[i, :lengths[i], :], dim=0) \n",
    "print(\"The max output has shape: \", max_pool.shape)\n",
    "print(\"The max output is: \")\n",
    "print(max_pool)\n",
    "print()\n",
    "\n",
    "# -------------------- #\n",
    "# Final representation #\n",
    "# -------------------- #\n",
    "# Concanate above three representations\n",
    "representations = torch.cat((last, mean_pool, max_pool), 1)\n",
    "print(\"The representation has shape: \", representations.shape)\n",
    "print(\"The representation is: \")\n",
    "print(representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tα αποτελέσματα με το LSTMPool σε 25 εποχές (στις 30 άρχιζε και έκανε overfit και το test loss αυξανόταν πολύ) με batch size ίσο με 120, ένα layer στο lstm και hidden layer size ίσο με 16 είναι τα παρακάτω:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"photos/LSTMpool_output.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   <tr>\n",
    "    <td> <img src=\"photos/LSTMpool_train.png\" alt=\"Drawing\" style=\"width: 650px;\"/> </td>\n",
    "    <td> <img src=\"photos/LSTMpool_test.png\" alt=\"Drawing\" style=\"width: 650px;\"/> </td>\n",
    "    </tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ερώτημα 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"photos/question3-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Χρησιμοποιήθηκε η υλοποίηση του μηχανισμού attention που μας δόθηκε [εδώ](https://gist.github.com/cbaziotis/94e53bdd6e4852756e0395560ff38aa4). Αποθηκεύτηκε ως μία νέα κλάση με όνομα __SelfAttention__ στον φάκελο με όλα τα μοντέλα models/. Το νέο μοντέλο μας τώρα, θα χρησιμοποιεί αυτό το attention layer για να αναπαριστά κάθε κείμενο ως το σταθμισμένο άθροισμα των word embeddings. Το νέο μοντέλο είναι το __NN_attention__ και ουσιαστικά ακολουθεί την λογική του BaselineDNN που δουλέψαμε στην προπαρασκευή με την διαφορά ότι η αναπαράσταση του κειμένου από τα word embeddings προκύπτει από το attention layer και μετά απλά προβάλλουμε την αναπαράσταση αυτή στις 3 κλάσεις που έχουμε."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tα αποτελέσματα με το NN_Attention σε 30 εποχές με batch size ίσο με 120 είναι τα παρακάτω:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"photos/NN_Attention_output.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   <tr>\n",
    "    <td> <img src=\"photos/NN_Attention_train.png\" alt=\"Drawing\" style=\"width: 650px;\"/> </td>\n",
    "    <td> <img src=\"photos/NN_Attention_test.png\" alt=\"Drawing\" style=\"width: 650px;\"/> </td>\n",
    "    </tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"photos/question3-2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Το μοντέλο αυτό είναι το __LSTM_Attention__ στον φάκελο models/ και η υλοποίησή του βασίστηκε στο πρώτο LSTM μοντέλο που φτιάξαμε (LSTMNet). Η μόνη διαφορά είναι ότι αντί να παίρνουμε ως αναπαράσταση για την κάθε πρόταση την τελευταία έξοδο του LSTM, παίρνουμε τις εξόδους του από το attention layer και η έξοδος του αποτελεί την αναπαράσταση της πρότασης. Στη συνέχεια, όπως κάθε φορά, την προβάλλουμε στον χώρο των κλάσεων και έχουμε τα παρακάτω αποτελέσματα:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tα αποτελέσματα με το LSTM_Attention σε 30 εποχές με batch size ίσο με 120, ένα layer στο lstm και hidden layer size ίσο με 16 είναι τα παρακάτω:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"photos/LSTM_Attention_output.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   <tr>\n",
    "    <td> <img src=\"photos/LSTM_Attention_train.png\" alt=\"Drawing\" style=\"width: 650px;\"/> </td>\n",
    "    <td> <img src=\"photos/LSTM_Attention_test.png\" alt=\"Drawing\" style=\"width: 650px;\"/> </td>\n",
    "    </tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ερώτημα 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"photos/question4-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Το νέο μοντέλο έχει αποθηκευτεί στο __models/__ με όνομα BiLSTMpool. Η μόνη αλλαγή που έγινε σε σχέση με το LSTMpool μοντέλο είναι ότι στην αρχικοποίηση του μοντέλου ορίστηκε το LSTM ως bidirectional και η είσοδος του projection layer ως διπλάσια από πριν καθώς τώρα κάθε output του LSTM έχει διπλάσιο μέγεθος. Αντίστοιχα, στο forward του μοντέλου άλλαξαν μόνο κάποιες διαστάσεις όπου διπλασιαστηκαν. Tα αποτελέσματα με το BiLSTMPool σε 30 εποχές με batch size ίσο με 120, ένα layer στο lstm και hidden layer size ίσο με 16 είναι τα παρακάτω __όπου έχει γίνει σημαντικό overfit στα δεδομένα μας__:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"photos/BiLSTM_output.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   <tr>\n",
    "    <td> <img src=\"photos/BiLSTMpool_train.png\" alt=\"Drawing\" style=\"width: 650px;\"/> </td>\n",
    "    <td> <img src=\"photos/BiLSTMpool_test.png\" alt=\"Drawing\" style=\"width: 650px;\"/> </td>\n",
    "    </tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"photos/question4-2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Σε αυτό το ερώτημα υλοποιήθηκε το μοντέλο BiLSTM_Attention στο οποίο η διαδικασία είναι ακριβώς ίδια με το LSTM_Attention με την μόνη διαφορά ότι διπλασιάζουμε την είσοδο στο Attention Layer. Tα αποτελέσματα με το BiLSTM_Attention σε 30 εποχές με batch size ίσο με 120, ένα layer στο lstm και hidden layer size ίσο με 16 είναι τα παρακάτω __όπου και εδώ έχουμε μεγάλο overfit__:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"photos/BiLSTM_Attention_output.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   <tr>\n",
    "    <td> <img src=\"photos/BiLSTM_Attention_train.png\" alt=\"Drawing\" style=\"width: 650px;\"/> </td>\n",
    "    <td> <img src=\"photos/BiLSTM_Attention_test.png\" alt=\"Drawing\" style=\"width: 650px;\"/> </td>\n",
    "    </tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ερώτημα 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"photos/question5-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για να αποθηκεύσουμε το μοντέλο μας χρησιμοποιούμε την εντολή torch.save(model, PATH) του Pytorch. Το μοντέλο, λοιπόν, με το χαμηλότερο loss στο validation set ήταν το LSTM μοντέλο με attention layer για hidden size ίσο με 16 και batch size ίσο με 128, το οποίο αποθηκεύτηκε ως __LSTM_Attention.pt__ στο φάκελο του project. Το αρχείο __predict-validation.py__ προβλέπει το validation set χρησιμοποιώντας το μοντέλο που δέχεται σαν όρισμα και αποθηκεύει τις προβλέψεις του στο __y_preds.txt__ (η υλοποίησή του εξηγείται με αναλυτικά σχόλια στον κώδικα)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"photos/question5-2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Θα χρησιμοποιήσουμε το NeAt-vision το οποίο βρίσκεται [εδώ](https://github.com/cbaziotis/neat-vision). Έχουμε ένα classification task οπότε το αρχείο με τα δεδομένα μας πρέπει να περιέχει τα εξής:\n",
    "<img src=\"photos/neat-vision.png\">\n",
    "Πρέπει, λοιπόν, για κάθε δείγμα να κατασκευάσουμε ένα python dictionary __sample__ και να το βάλουμε σε μία λίστα με όνομα __data__. Το dictionary παίρνει για κάθε δείγμα τις παραπάνω τιμές ενώ για id βάζω το index του. Στο τέλος, για να μετατρέψω μία λίστα με λεξικά σε json μορφή χρησιμοποίησα την βιβλιοθήκη json. Όλα αυτά γίνονται στο αρχείο __predict-json.py__, αποτέλεσμα του οποίου είναι το αρχέιο __data.json__. Ακόμη, δημιουργήθηκε και ένα αρχείο __labels.json__ το οποίο περιέχει την ερμηνεία των τριών labels και μία σύντομη περιγραφή τους. Τα δύο αυτά αρχείο μπορούν να εισαχθούν στο [neat-vision](https://cbaziotis.github.io/neat-vision/) και να παρατηρήσουμε την κατανομή των βαρών του attention layer και τις προβλέψεις με έναν όμορφο και κατανοητό τρόπο. __Αυτό που περιμένουμε να δούμε είναι ότι σε μία πρόταση οι λέξεις που καθορίζουν το νόημά της να έχουν υψηλό score__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ενδεικτικά βλέπουμε παρακάτω τα αποτελέσματα σε ορισμένα tweets που εκτιμήθηκαν σωστά."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __positive__\n",
    "<table>\n",
    "    <tr>\n",
    "    <td> <img src=\"photos/neat-vision/positive/1.svg\" alt=\"Drawing\" style=\"width: 750px;\"/> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td> <img src=\"photos/neat-vision/positive/2.svg\" alt=\"Drawing\" style=\"width: 800px;\"/> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td> <img src=\"photos/neat-vision/positive/3.svg\" alt=\"Drawing\" style=\"width: 800px;\"/> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td> <img src=\"photos/neat-vision/positive/4.svg\" alt=\"Drawing\" style=\"width: 800px;\"/> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αυτό που παρατηρούμε είναι ότι τα score του attention layer σε λέξεις με θετική σημασία όπως 'love', 'Congratulations', 'Thank you', 'kind' (ή και σύμβολα όπως ! ή τα emojis) είναι υψηλά με αποτέλεσμα να θεωρηθούν σημαντικά και να καθορίσουν όλη την σημασία της πρότασης."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __negative__\n",
    "<table>\n",
    "    <tr>\n",
    "    <td> <img src=\"photos/neat-vision/negative/1.svg\" alt=\"Drawing\" style=\"width: 750px;\"/> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td> <img src=\"photos/neat-vision/negative/2.svg\" alt=\"Drawing\" style=\"width: 750px;\"/> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td> <img src=\"photos/neat-vision/negative/3.svg\" alt=\"Drawing\" style=\"width: 750px;\"/> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td> <img src=\"photos/neat-vision/negative/4.svg\" alt=\"Drawing\" style=\"width: 750px;\"/> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αντίστοιχα εδώ έχουμε μεγάλα score σε λέξεις με αρνητική σημασία όπως 'Disgraceful', 'killed', 'naive', 'Morons', 'worst'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __neutral__\n",
    "<table>\n",
    "    <tr>\n",
    "    <td> <img src=\"photos/neat-vision/neutral/1.svg\" alt=\"Drawing\" style=\"width: 750px;\"/> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td> <img src=\"photos/neat-vision/neutral/2.svg\" alt=\"Drawing\" style=\"width: 750px;\"/> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td> <img src=\"photos/neat-vision/neutral/3.svg\" alt=\"Drawing\" style=\"width: 650px;\"/> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td> <img src=\"photos/neat-vision/neutral/4.svg\" alt=\"Drawing\" style=\"width: 750px;\"/> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στις neutral προτάσεις δεν υπάρχουν λέξεις με μεγάλο score (ή υπάρχουν αλλά είναι ουδέτερες), πράγμα αναμενόμενο αφού τέτοιες προτάσεις δεν θα εμπεριέχουν λέξεις με έντονο συναίσθημα και σημασία."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Μπορούμε, ακόμη, να δούμε τα λάθη που έκανε το μοντέλο μας και τα αντίστοιχα scores που έδωσε στις λέξεις. Πολλές φορές αυτό συνέβη επειδή η πρόταση περιέχει θετικές ή αρνητικές λέξεις ενώ το συνολικό της νόημα είναι ουδέτερο (το μοντέλο δηλαδή έδωσε σημασία στην θετική λέξη η οποία όμως στο context που ήταν δεν επηρέαζε τόσο). Βλέπουμε παρακάτω κάποια παραδείγματα:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __neutral__\n",
    "<table>\n",
    "    <tr>\n",
    "    <td> <img src=\"photos/neat-vision/misses/neg-neu.png\" alt=\"Drawing\" style=\"width: 950px;\"/> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td> <img src=\"photos/neat-vision/misses/neg-neu2.png\" alt=\"Drawing\" style=\"width: 950px;\"/> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td> <img src=\"photos/neat-vision/misses/pos-neu.png\" alt=\"Drawing\" style=\"width: 950px;\"/> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ακόμη, υπάρχουν περιπτώσεις όπου συνηπάρχουν θετικές και αρνητικές λέξεις και προβέπει θετικό label ενώ είναι αρνητικό ή αντίθετα, όπως στο παρακάτω όπου υπάρχει η λέξη thanks και η λέξη retards, αλλά το μοντέλο δεν καταλαβαίνει ότι πρόκειται για ειρωνία."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"photos/neat-vision/misses/pos-neg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"photos/question5-3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αρχικά τρέχοντας __python main.py -model=NN_Attention -save=True__ αποθηκεύουμε το μοντέλο που εκπαιδεύτηκε στο ερώτημα 3.1 στο NN_Attention.pt. Στη συνέχεια, με την εντολή __python predict-json NN_Attention data3-1.json__ αποθηκεύουμε όλα αυτά που χρειαζόμαστε για το neat-vision σε json μορφή. Τώρα, λοιπόν, μπορούμε με το neat-vision να παρατηρήσουμε τις διαφορές των δύο μοντέλων. Αυτό που παρατηρούμε είναι ότι το LSTM κάνει περισσότερες σωστές προβλέψεις από το απλό NN γιατί κρατάει πληροφορία για όλη την προηγούμενη πρόταση. Έτσι, δεν ενδιαφέρεται μόνο για μεμονομένες λέξεις με θετική ή αρνητική σημασία αλλά και για το όλο context που έχουν πίσω τους. Ακολουθούν τρία παραδείγματα που αναδεικνύουν αυτό το χαρακτηριστικό μαζί με έναν σύντομο σχολιασμό."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"photos/neat-vision/diff/emb1.png\" alt=\"Drawing\" style=\"width: 950px;\"/>\n",
    "<img src=\"photos/neat-vision/diff/lstm1.png\" alt=\"Drawing\" style=\"width: 950px;\"/>\n",
    "\n",
    "Βλέπουμε ότι LSTM στη λέξη 'love' έδωσε διπλάσιο βάρος από το απλό νευρωνικό διότι έχει προηγηθεί πιο πριν η έκφραση 'so much' το οποίο ενισχύει ακόμα πιο πολύ την θετική έννοια της λέξης 'love'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"photos/neat-vision/diff/emb2.png\" alt=\"Drawing\" style=\"width: 950px;\"/>\n",
    "<img src=\"photos/neat-vision/diff/lstm2.png\" alt=\"Drawing\" style=\"width: 950px;\"/>\n",
    "\n",
    "Εδώ, αντίστοιχα, η λέξη 'analogy' πήρε μεγαλύτερο βάρος έχοντας το 'excellent' πιο πριν στο lstm που λαμβάνει υπόψιν το context της λέξης απ'ότι στο νευρωνικό που βλέπει τις λέξεις σαν ξεχωριστά tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"photos/neat-vision/diff/emb4.png\" alt=\"Drawing\" style=\"width: 950px;\"/>\n",
    "<img src=\"photos/neat-vision/diff/lstm4.png\" alt=\"Drawing\" style=\"width: 950px;\"/>\n",
    "\n",
    "Και εδώ βλέπουμε ότι όταν λαμβάνουμε υπόψιν το context η λέξη 'team' μετά την λέξη 'best' έχει πολύ θετική σημασία πράγμα που το βλέπει το LSTM και κάνει σωστή πρόβλεψη. Αντίθετα, με τον αρχικό τρόπο η λέξη 'best' σωστά λαμβάνει μεγάλο score αλλά η λέξη 'team' που την ακολοθεί αφού αντιμετωπίζεται ξεχωριστά έχει χαμηλό score και συνολικά δεν έχει σημασία για την πρόταση."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ερώτημα 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"photos/question6-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Θα χρησιμοποιηθεί ο __[Tfidf Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)__ της βιβλιθήκης sklearn ο οποίος μετατρέπει δεδομένα από raw text μορφή σε έναν πίνακα με τα TF-IDF features. Συνοπτικά αναφέρεται ότι για κάθε λέξη υπολογίζονται τα παρακάτω:\n",
    "- __Term Frequency (tf):__ Είναι η συχνότητα εμφάνισης της λέξης στο συγκεκριμένο sample.\n",
    "- __Data Frequency (df):__ Είναι ο αριθμός των διαφορετικών samples που περιέχουν την συγκεκριμένη λέξη.\n",
    "- __Inverse Data Frequency (idf):__ Είναι ο λογάριθμος του λόγου των συνολικών samples προς to df της συγκεκριμένης λέξης."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Συνεπώς, το tf-idf score της κάθε λέξης προκύπτει από το γινόμενο του tf επί του idf με αποτέλεσμα όσο πιο συχνά εμφανίζεται μία λέξη σε ένα sample να έχει μεγαλύτερο βάρος (και κατ'επέκταση μεγαλύτερη σημασία) αλλά όσο περισσότερο εμφανίζεται και σε διαφορετικα samples, το βάρος της να πέφτει (πχ λέξεις όπως 'the', 'and' εμφανίζονται παντού αλλά δεν έχουν κάποια σημασία). Περισσότερα έχουν αναφερθεί στο 1ο εργαστήριο, οπότε παρακάτω θα χρησιμοποιήσουμε τον Tfidf Vectorizer σε ένα απλό δείγμα:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      "['I am Greek', 'I am 22 years old', 'I study in NTUA']\n",
      "Tokenized input: \n",
      "[['I', 'am', 'Greek'], ['I', 'am', '22', 'years', 'old'], ['I', 'study', 'in', 'NTUA']]\n",
      "[[0.         0.72033345 0.42544054 0.         0.54783215 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.50461134 0.         0.29803159 0.         0.38376993 0.\n",
      "  0.50461134 0.         0.50461134]\n",
      " [0.         0.         0.32274454 0.54645401 0.         0.54645401\n",
      "  0.         0.54645401 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# A sample input \n",
    "X = ['I am Greek', 'I am 22 years old', 'I study in NTUA']\n",
    "print(\"Input: \")\n",
    "print(X)\n",
    "# Tokenize input\n",
    "social_tokenizer = SocialTokenizer().tokenize\n",
    "data = [social_tokenizer(example) for example in X]\n",
    "print(\"Tokenized input: \")\n",
    "print(data)\n",
    "# Define tfidf tokenizer and use it on tokenized data\n",
    "tfidf = TfidfVectorizer(tokenizer=lambda i:i, lowercase=False)\n",
    "data_tfidf = tfidf.fit_transform(data)\n",
    "print(data_tfidf.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η ιδέα ήταν να εξάγω τα BoW χαρακτηριστικά για κάθε πρόταση και να τα κάνω concatenate με την έξοδο του LSTM και τελικά αυτή η αναπαράσταση να προβάλλεται στον χώρο των κλάσεων. Αλλά, επειδή οι πίνακες προφανώς ήταν τεράστιοι και γέμιζε η RAM και δεν κατάφερα με το pytorch να χειριστώ αραιούς πίνακες, η λειτουργία αυτή δεν υλοποιήθηκε. Υποθέτω, βέβαια, ότι η επίδοση του μοντέλου δεν θα βελτιωνόταν."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"photos/question6-2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Σε περιπτώσεις όπου το dataset είναι μικρό και το context είναι domain specific. Αυτό γιατί τα word embeddings θα είναι προεκπαιδευμένα σε διαφορετικό context συνήθως και πιο γενικό με αποτέλεσμα η σημασία και νοηματική σχέση των λέξεων να μην εκφράζεται σωστά στο συγκεκριμένο domain του dataset. Αντίθετα, χρησιμοποιώντας BoW παίρνουμε πιο συγκεκριμένη πληροφορία για τις λέξεις σε αυτό το context και παράλληλα αφού το dataset είναι μικρό δεν έχουμε θέμα με τους αραιούς πίνακες που θα δημιουργηθούν."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Πρόσθετες λειτουργίες"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Στο πρόγραμμα προστέθηκε και ένα parser μέσω της βιβλιοθήκης __argparse__ της python για να δίνονται οι διάφορες παράμετροι μέσω του standard input. Ουσιαστικά υποστηρίζονται οι παρακάτω παράμετροι κανένας από τους οποίους δεν είναι υποχρεωτικός:\n",
    "    - __-embedding__ στο οποίο μπορεί ο χρήστης να ορίσει το embedding που επιθυμεί ανάμεσα σε αυτά που περιέχονται στον φάκελο embeddings/ με default να είναι τα twitter embeddings των 50 διαστάσεων.\n",
    "    - __-emb_size__ στο οποίο ο χρήστης εισάγει το μέγεθος των παραπάνω embedding (default τιμή το 50).\n",
    "    - __-epochs__ στο οποίο ο χρήστης ορίζει τον αριθμό των εποχώς που θα εκπαιδευτεί το μοντέλο (default τιμή οι 30 εποχές).\n",
    "    - __-batch_size__ στο οποίο ο χρήστης μπορεί να ορίσει το μέγεθος του κάθε mini batch (default τιμή το 128 ενώ συστήνεται να είναι δύναμη του 2 για λόγους ταχύτητας).\n",
    "    - __-model__ στο οποίο εισάγει το μοντέλο που θέλει να χρησιμοποιήσει για την εκπαίδευση πάνω στα δεδομένα μας από τον φάκελο model/ (default το BaselineDNN που είναι ένα απλό NN).\n",
    "    - __-save__ το οποίο αν είναι True αποθηκεύει το μοντέλο μετά την εκπαίδευση σε αρχείο .pt (default το False)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Προστέθηκε ένα αρχείο Structure.pdf που περιγράφει την σημασία ορισμένων αρχείων για ευκολότερη κατανόηση της δομής του project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
