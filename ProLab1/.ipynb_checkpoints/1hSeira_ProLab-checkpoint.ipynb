{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center> Προσπαρασκευή 1ης εργαστηριακής άσκησης:</center></h1>\n",
    "<h2><center> Μέρος πρώτο: Ορθογράφος</center><h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ΕΚΤΕΛΕΣΗ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 1: Κατασκευή corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__α)__ Κατεβάζουμε λοιπόν από το project Gutenberg το βιβλίο __War of the Worlds__ σε plain txt μορφή και το αποθηκεύουμε με το όνομα __War.txt__ (με το ! στην αρχή μιάς εντολής δηλώνουμε ότι ακολουθεί ένα shell command). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget  -c http://www.gutenberg.org/files/36/36-0.txt -O War.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__β)__ Προφανώς μπορούμε να ενώσουμε περισσότερα βιβλία για την κατασκευή ενός μεγαλύτερου corpus. Τα πλεονεκτήματα αυτής της τεχνικής (εκτός από την αύξηση του μεγέθους των δεδομένων) είναι:\n",
    "1. Τα δεδομένα μας θα προέρχονται από πολλές διαφορετικές πηγές με αποτέλεσμα το γλωσσικό μοντέλο μας να γενικεύσει περισσότερο και να μην κάνει overfit στα δεδομένα μας.\n",
    "2. Όταν έχουμε ένα μεγάλο corpus τα στατιστικά στοιχεία που θα υπολογίσουμε πάνω σε αυτό θα είναι ακριβέστερα και ακόμα θα είμαστε σε θέση να κρίνουμε ποιό στατιστικό στοιχείο είναι σημαντικότερο για το μοντέλο μας."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 2: Προεπεξεργασία corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Έχουμε λοιπόν το __corpus__ που θα χρησιμοποιήσουμε στον ίδιο φάκελο με το notebook μας. Θα ορίσουμε τώρα ορισμένες χρήσιμες συναρτήσεις για την προεπεξεργασία του."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__α)__ Η συνάρτηση __identity_preprocess__ διαβάζει ένα string και απλά γυρνάει τον εαυτό του."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_preprocess(string_var):\n",
    "    return string_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__β)__ Η συνάρτηση __read_path__ δέχεται σαν όρισμα το path του αρχείου που θέλουμε να διαβάσουμε και μία συνάρτηση preprocess και διαβάζει το αρχείο γραμμή προς γραμμή σε μία λίστα, καλώντας την preprocess σε κάθε γραμμή. Χρησιμοποιούμε σαν default όρισμα για την preprocess την identity_preprocess που ορίστηκε παραπάνω."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_path(file_path, preprocess = identity_preprocess):\n",
    "    #initilize the list of processed lines\n",
    "    processed_lines = []\n",
    "    #open file to read mode\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            #omit spaces\n",
    "            if not line.isspace():\n",
    "                processed_lines.extend(preprocess(line))\n",
    "    return processed_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__γ)__ Η συνάρτηση __tokenize__ δέχεται σαν όρισμα ένα string s και: \n",
    "    1. καλεί την strip() και lower() πάνω στο s, \n",
    "    2. αφαιρεί όλα τα σημεία στίξης / σύμβολα / αριθμούς, αφήνωντας μόνο αλφαριθμητικούς χαρακτήρες, \n",
    "    3. αντικαθιστά τα newlines με κένα, \n",
    "    4. κάνει split() τις λέξεις στα κενά."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "\n",
    "def tokenize(s):\n",
    "    #Remove possible spaces from the start or the end of the string and\n",
    "    #turn all letters lowercase.\n",
    "    s = s.strip().lower()\n",
    "    #Remove all punctuations, symbols and numbers from the string leaving\n",
    "    #only lowercase alphabetical letters.\n",
    "    s = \"\".join((char for char in s if char not in string.punctuation and not char.isdigit()))\n",
    "    #Replace new line characters with spaces\n",
    "    s = s.replace('\\n',' ')\n",
    "    #Split the string in every space resulting in a list of tokens\n",
    "    res = s.split(\" \")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__δ)__ Μπορούμε αντί να κατασκευάσουμε τον δικό μας tokenizer να χρησιμοποιήσουμε έναν έτοιμο από την βιβλιοθήκη nltk που διαθέτει πόλλους διαφορετικούς tokenizer. Παρακάτω ορίζουμε έναν από αυτούς τον οποίο θα συγκρίνουμε αργότερα με τον δικό μας."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "def nltk_tokenizer(s):\n",
    "    words = WordPunctTokenizer().tokenize(text = s)\n",
    "    words = [word.lower() for word in words if word.isalpha() and not word.isdigit()]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στη συνέχεια μπορούμε να συγκρίνουμε τους δύο tokenizers δίνοντας τους ως είσοδο ένα string με διάφορα είδη χαρακτήρων, συμβόλων και αριθμών. Παρατηρούμε ότι και οι δύο ανταποκρίνονται πολύ καλά δίνοντάς μας το επιθυμητό αποτέλεσμα, απλά ο δικός μας αποθηκεύει και κάποια κενά ως token το οποίο μπορεί να διορθωθεί κατά την δημιουργία του λεξικού."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"hi my !@( name 32 is ... Panos\"\n",
    "print(\"My tokenizer :\" + \" \", end=\"\")\n",
    "print(tokenize(test_string))\n",
    "print(\"Nltk tokenizer :\" + \" \", end=\"\")\n",
    "print(nltk_tokenizer(test_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 3: Κατασκευή λεξικού και αλφαβήτου"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__α)__ Δημιουργούμε μία λίστα η οποία θα περιέχει όλα τα μοναδικά tokens που βρίσκονται στο corpus που θα χρησιμοποιήσουμε, σύμφωνα με τον tokenizer του Βήματος 2. Ουσιαστικά θα καλέσουμε την συνάρτηση read_path με όρισμα τον tokenizer που θέλουμε να εφαρμοστεί σε κάθε γραμμή του αρχείου και στη συνέχεια θα πάρουμε μόνο τις μοναδικές λέξεις για να σχηματίσουμε το λεξικό μας."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(file_path):\n",
    "    tokens = read_path(file_path, tokenize)\n",
    "    distinct_tokens = list(dict.fromkeys(tokens))\n",
    "    return distinct_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_dict = get_tokens(\"War.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__β)__ Δημιουργούμε μία λίστα που περιέχει το αλφάβητο του corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alphabet(tokens):\n",
    "    alphabet = []\n",
    "    for token in tokens:\n",
    "        alphabet.extend(list(token))\n",
    "    alphabet = list(dict.fromkeys(alphabet))\n",
    "    return alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = get_alphabet(tokens_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 4: Δημιουργία συμβόλων εισόδου-εξόδου"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η συνάρτηση __alphabet _to_int__ δέχεται ως όρισμα το αλφάβητο που έχουμε ορίσει από το corpus, αντιστοιχίζει κάθε χαρακτήρα σε ένα αύξοντα ακέραιο index και γράφει το αποτέλεσμα σε ένα αρχείο chars.syms με αυτή τη μορφή http://www.openfst.org/twiki/pub/FST/FstExamples/ascii.syms. Σε όλα τα επόμενα βήματα θα χρησιμοποιήσουμε το αρχείο chars.syms για τα σύμβολα εισόδου και εξόδου των FSTs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphabet_to_int(alphabet):\n",
    "    # open file\n",
    "    f = open(\"chars.syms\", \"w\")\n",
    "    # match epsilon to 0\n",
    "    f.write(\"EPS\" + 7*\" \" + str(0) + '\\n')\n",
    "    num = 21\n",
    "    for character in alphabet:\n",
    "        # match every other character to an increasing index\n",
    "        f.write(character + 7*\" \" + str(num) + '\\n')\n",
    "        num += 1\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet_to_int(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 5: Κατασκευή μετατροπέων FST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__α)__ Για να κατασκευάσουμε τον μετατροπέα σε αυτό το βήμα αλλά και τον αποδοχέα στο επόμενο θα χρειαστούμε μία συνάρτηση format_arc η οποία θα διαμορφώνει μία γραμμή από το αρχείο περιγραφής του κάθε FST. Συγκεκριμένα δέχεται ως όρισμα τα __src__, __dest__, __ilabel__, __olabel__ και το __weight__ (με default τιμή το 0) και τα επιστρέφει στην κατάλληλη μορφή όπως αναφέρεται και εδώ http://www.openfst.org/twiki/bin/view/FST/FstQuickTour#CreatingFsts/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_arc(src, dest, ilabel, olabel, weight=0):\n",
    "    return (str(src) + \" \" + str(dest) + \" \" + str(ilabel) + \" \" + str(olabel) + \" \" + str(weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα λοιπόν θα διαμορφώσουμε το αρχείο περιγραφής του μετατροπέα μας, ο οποίος θα έχει μία κατάσταση και θα υλοποιεί την απόσταση Levenshtein αντιστοιχίζοντας:\n",
    "1. Κάθε χαρακτήρα στον εαυτό του με βάρος 0 (no edit).\n",
    "2. Κάθε χαρακτήρα στο EPS με βάρος 1 (deletion).\n",
    "3. Το EPS σε κάθε χαρακτήρα με βάρος 1 (insertion).\n",
    "4. Κάθε χαρακτήρα σε κάθε άλλο χαρακτήρα με βάρος 1 (substitution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η περιγραφή του μετατροπέα αποθηκεύεται στο αρχείο __transducer.fst__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get alphabet of the corpus\n",
    "alphabet = get_alphabet(get_tokens(\"War.txt\"))\n",
    "# open file to write mode\n",
    "f = open(\"transducer.fst\", \"w\")\n",
    "for letter in alphabet:\n",
    "    # no edit\n",
    "    f.write(format_arc(0, 0, letter, letter) + \"\\n\")\n",
    "    # deletion\n",
    "    f.write(format_arc(0, 0, letter, \"EPS\", 1) + \"\\n\")\n",
    "    # insertion\n",
    "    f.write(format_arc(0, 0, \"EPS\", letter, 1) + \"\\n\")\n",
    "for i in range(len(alphabet)):\n",
    "    for j in range(len(alphabet)):\n",
    "        if i != j:\n",
    "            # substitution\n",
    "            f.write(format_arc(0, 0, alphabet[i], alphabet[j], 1) + \"\\n\")\n",
    "\n",
    "# make initial state also final state\n",
    "f.write(\"0\")\n",
    "# close file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τρέχοντας λοιπόν το παρακάτω shell command κάνουμε compile τον μετατροπέα μας. Το binary αρχείο που προκύπτει με όνομα __transducer.bin.fst__ είναι αυτό που θα χρησιμοποιήσουμε στις επόμενες λειτουργίες."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./compile_transducer.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Είναι προφανές ότι ο μετατροπέας μας αυτή την στιγμή σε μία λέξη εισόδου αν πάρουμε το shortest path απλά θα επιστρέψει την ίδια την λέξη αυτούσια χωρίς καμμία αλλαγή. Και αυτό γιατί για κάθε σύμβολο του αλφαβήτου έχουμε μετάβαση στον εαυτό του με κόστος 0. Ο μετατροπέας μας θα αποκτήσει νόημα όταν τον συνθέσουμε με τον αποδοχέα του επόμενου ερωτήματος."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__β)__ Αυτός είναι ένας αρκετά αφελής τρόπος για τον υπολογισμό των βαρών για κάθε edit. Αν τώρα είχαμε στη διάθεση μας ό,τι δεδομένα θέλουμε αυτό που θα κάναμε είναι ότι θα υπολογίζαμε τα βάρη με βάση το πόσο συχνά γίνεται αυτό το λάθος. Πιο συγκεκριμένα, θα υπολογίζαμε για κάθε σύμβολο του αλφαβήτου την πιθανότητα κάποιος να το διαγράψει, να το προσθέσει ή να το αντικαταστήσει με κάποιο άλλο. Στη συνέχεια, θα μετατρέπαμε αυτές τις πιθανότητες σε κόστη παίρνοντας τον αρνητικό λογάριθμο και θα είχαμε τα τελικά βάρη μας για κάθε σύμβολο στο deletion και το insertion και για κάθε δυάδα συμβόλων στο substitution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 6: Κατασκευή αποδοχέα λεξικού"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__α)__ Σε αυτό το βήμα θέλουμε να κατασκευάσουμε έναν αποδοχέα με μία αρχική κατάσταση που αποδέχεται κάθε κατάσταση του λεξικού μας. Τα βάρη όλων των ακμών θα είναι 0. Αντίστοιχα με πριν γράφουμε την περιγραφή του αποδοχέα σε ένα αρχείο __acceptor.fst__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tokens of the corpus (our acceptor should accept only these words)\n",
    "tokens = get_tokens(\"War.txt\")\n",
    "# open file to write mode\n",
    "f = open(\"acceptor.fst\", \"w\")\n",
    "s = 1\n",
    "for token in tokens:\n",
    "    letters = list(token)\n",
    "    for i in range(0, len(letters)):\n",
    "        if i == 0:\n",
    "            # For each token make state 1 its first state\n",
    "            f.write(format_arc(1, s+1, letters[i], letters[i]) + \"\\n\")\n",
    "        else:\n",
    "            f.write(format_arc(s, s+1, letters[i], letters[i]) + \"\\n\")\n",
    "        s += 1\n",
    "        if i == len(letters) - 1:\n",
    "            # When reaching the end of a token go to final state 0 though an ε-transition\n",
    "            f.write(format_arc(s, 0, \"EPS\", \"EPS\") + \"\\n\")\n",
    "# make state 0 final state\n",
    "f.write(\"0\")\n",
    "# close the file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Στη συνέχεια το fst μας γίνεται compile στο αρχείο __acceptor.bin.fst__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./compile_acceptor.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__β)__ Στη συνέχεια καλούμε τις συναρτήσεις fstrmepsilon, fstdeterminize και fstminimize για να βελτιστοποιήσουμε το μοντέλο μας. Παρακάτω αναφέρεται η λειτουργία της κάθεμίας:\n",
    "1. __fstrmepsilon__: Μετατρέπει το FST σε ένα ισοδύναμο που να μην περιέχει ε-μεταβάσεις\n",
    "2. __fstdeterminize__: Μετατρέπει το FST σε ένα ισοδύναμο ντετερμινιστικό. Το τελικό FST, δηλαδή, θά έχει για κάθε κατάσταση και για κάθε σύμβολο εισόδου μία και μόνο μετάβαση.\n",
    "2. __fstminimize__: Ελαχιστοποιεί το FST ώστε να έχει τον ελάχιστο αριθμό καταστάσεων."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! fstrmepsilon acceptor.bin.fst acceptor.bin.fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! fstdeterminize acceptor.bin.fst acceptor.bin.fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! fstminimize acceptor.bin.fst acceptor.bin.fst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 7: Κατασκευή ορθογράφου"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στο βήμα αυτό θα συνθέσουμε τον Levenshtein transducer με τον αποδοχέα του ερωτήματος 6α παράγοντας τον min edit distance spell checker. Αρχικά θα ταξινομήσουμε τις εξόδους του transducer και τις εισόδους του αποδοχέα με την συνάρτηση __fstarcsort__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! fstarcsort --sort_type=olabel transducer.bin.fst transducer_sorted.fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! fstarcsort --sort_type=ilabel acceptor.bin.fst acceptor_sorted.fst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στη συνέχεια συνθέτουμε τον transducer με τον αποδοχέα με την συνάρτηση fstcompose αποθηκεύοντας τον min edit distance spell checker στο αρχείο __spell_checker.fst__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! fstcompose transducer_sorted.fst acceptor_sorted.fst spell_checker.fst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η συμπεριφορά του μετατροπέα που έχουμε υλοποιήσει διαφέρει ανάλογα με τα βάρη που θα βάλουμε σε κάθε edit. Συγκεκριμένα:\n",
    "1. Στην περίπτωση που τα edits είναι ισοβαρή έχουμε ένα αφελή transducer που απλά διορθώνει τις λέξεις χωρίς να λαμβάνει υπόψιν του κάποια γλωσσική πληροφορία, με μόνο κριτήριο να κάνει τις ελάχιστες δυνατές μετατροπές στην λέξη εισόδου.\n",
    "2. Στην περίπτωση τώρα που τα edits δεν είναι ισοβαρή μπορούμε να υλοποιήσουμε έναν πιο ισχυρό trandsucer. Συγκεκριμένα, μπορούμε να ορίσουμε με βάση κάποια γνωστά μοντέλα την πιθανότητα ένα σύμβολο να λείπει, να έχει προστεθεί λανθασμένα ή να έχει αντικατασταθεί από κάποιο άλλο σύμβολο και να μετατρέψουμε αυτές τις πιθανότητες σε κόστη τα οποία θα μεταφραστούν στη συνέχεια ως βάρη στα αντίστοιχα edits. Έτσι, η λέξη θα διορθώνεται όχι με βάση τις ελάχιστες δυνατές μετατροπές αλλά με το ποιά λέξη είναι πιο πίθανο να ήθελε να γράψει αυτός που την έγραψε λάθος."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__β)__  Στην περίπτωση, λοιπόν, που η είσοδος στον min edit spell checker μας είναι η λέξη __cit__ θα προσπαθήσει με τον ελάχιστο αριθμό που τις τρεις λειτουργίες που του επιτρέπονται (insert, delete, substitute) να την μετατρέψει σε μία λέξη που υπάρχει στο λεξικό μας. Επειδή, βέβαια, το λεξικό μας προέρχεται απλά από ένα βιβλίο είναι λογικό πολλές δημοφιλής λέξεις να λείπουν. Στην περίπτωση μας έχουμε τις εξής πιθανές προβλέψεις:\n",
    "1. Με μία αλλαγή μόνο:\n",
    "  * __cat__\n",
    "  * __sit__\n",
    "  * __fit__\n",
    "  * __cut__\n",
    "  * __cite__\n",
    "2. Σε περίπτωση τώρα που καμία από τις παραπάνω λέξεις δεν βρίσκεται στο λεξικό μας ο ορθογράφος θα κοιτάξει στις λέξεις που προκύπτουν από δύο αλλαγές στις λέξεις κ.ο.κ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 8: Αξιολόγηση ορθογράφου"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__α)__ Για να κάνουμε το evaluation του ορθογράφου κατεβάζουμε το παρακάτω σύνολο δεδομένων:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/georgepar/python-lab/master/spell_checker_test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__β)__ Από το παραπάνω test set λοιπόν θα επιλεξουμε 20 λέξεις στην τύχη και με βάση τον αλγόριθμο των ελάχιστων μονοπατιών στο γράφο του μετατροπέα του βήματος 7 θα την διορθώσουμε και θα εκτιμήσουμε την διόρθωση."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Δημιουργούμε αρχικά μία συνάρτηση __predict__ η οποία δέχεται μία λέξη που πρέπει να διορθωθεί και γράφει σε ένα αρχείο __pred_word.fst__ την περιγραφή ενός FST το οποίο αποδέχεται την συγκεκριμένη λέξη. Το FST αυτό θα το κάνουμε στη συνέχεια compose με τον ορθογράφο μας για να πάρουμε το τελικό αποτέλεσμα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(word):\n",
    "    s= 1\n",
    "    letters = list(word)\n",
    "    # open file to write mode\n",
    "    f = open(\"pred_word.fst\", \"w\")\n",
    "    for i in range(0, len(letters)):\n",
    "        # for each letter of the word make a transition with zero weight\n",
    "        f.write(format_arc(s, s+1, letters[i], letters[i], 0) + '\\n')\n",
    "        s += 1\n",
    "        if i == len(letters) - 1:\n",
    "            # when reaching the end the word make a ε-transition to the final state 0 \n",
    "            f.write(format_arc(s, 0, \"EPS\",  \"EPS\", 0) + '\\n')\n",
    "    # final state\n",
    "    f.write(\"0\")\n",
    "    # close the file\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στη συνέχεια, επιλέγουμε από το test set 20 τυχαίες λέξεις. Συγκεκριμένα, επιλέγουμε 20 τυχαίες σειρές και από κάθε σειρά επιλέγουμε τυχαία μία λανθασμένη λέξη. Βάζουμε ένα συγκεκριμένο seed έτσι ώστε να προκύπτουν οι ίδιες λέξεις κάθε φορά και να γίνει η εκτίμηση αργότερα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1)\n",
    "test_words = []\n",
    "for _ in range(20):\n",
    "    random_lines = random.choice(open('spell_checker_test_set').readlines())\n",
    "    test_words.append(random.choice(random_lines.strip('\\n').split()[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τέλος, για κάθε λέξη καλούμε την συνάρτηση predict και στη συνέχεια τρέχουμε στο bash (με το σύμβολο !) το αρχείο predict.sh δίνοντας ως όρισμα τον ορθογράφο μας."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in test_words:\n",
    "    print(word + \":\" + \" \",end='')\n",
    "    predict(word)\n",
    "    ! ./predict.sh spell_checker.fst\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Σχολιασμός αποτελεσμάτων: \n",
    "Παρατηρούμε ότι ο ορθογράφος λειτουργεί με μικρή ακρίβεια. Υπάρχουν συγκεκριμένα λάθη σε κάποιες λέξεις που οφείλονται στο γεγονός ότι τα βάρη στον μετατροπέα είναι όλα 1 και ότι το corpus μας αποτελείται μόνο από ένα βιβλίο.\n",
    "1. __chosing__ το οποίο διορθώνεται σε chasing αλλά θα μπορούσε να διορθωθεί σε choosing το οποίο έχει ίδιο αριθμό αλλαγών με το chasing αλλά θα μπορύσαμε να πούμε ότι όταν δύο γράμματα είναι συνεχόμενα είναι πολύ πιθανό να παραλειφθεί το ένα από τα δύο. Άρα το λάθος αυτό οφείλεται στά ίδια βάρη.\n",
    "2. __aranging__, __awfall__, __defenition__, __compair__, __transfred__, __adress__, __juise__ που θα μπορούσαν να γίνουν με λιγότερα edits σε arranging,awful, definition, compare, transfered, address, juice αντίστοιχα. Τα λάθη αυτά οφείλονται στο ότι οι συγκεκριμένες λέξεις δεν υπήρχας στο corpus μας."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 9: Εξαγωγή αναπαραστάσεων word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__α)__ Αρχικά θα διαβάσουμε το κείμενο σε μία λίστα από tokenized προτάσεις χρησιμοποιώντας τον κώδικα του βήματος 2γ. Για να χωρίσουμε το corpus σε προτάσεις χρησιμοποιούμε την βιβλιοθήκη __nltk__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# We split the corpus in a list of tokenized sentences.\n",
    "file_path = \"War.txt\"\n",
    "tokenized_sentences = []\n",
    "with open(file_path, \"r\") as f:\n",
    "    text = f.read()\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    tokenized_sentences = [tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__β)__ Θα χρησιμοποιήσουμε την κλάση Word2Vec του gensim για να εκπαιδεύσουμε 100-διάστατα word2vec embeddings με βάση τις προτάσεις του βήματος 9α. Θα χρησιμοποιήσουμε window = 5 και 1000 εποχές."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Initialize word2vec. Context is taken as the 2 previous and 2 next words\n",
    "model = Word2Vec(tokenized_sentences, window=5, size=100, workers=4)\n",
    "# Train the model for 1000 epochs\n",
    "model.train(tokenized_sentences, total_examples=len(sentences), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ordered vocabulary list\n",
    "voc = model.wv.index2word\n",
    "# get vector size\n",
    "dim = model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert to numpy 2d array (n_vocab x vector_size)\n",
    "def to_embeddings_Matrix(model):  \n",
    "    embedding_matrix = np.zeros((len(model.wv.vocab), model.vector_size))\n",
    "    for i in range(len(model.wv.vocab)):\n",
    "        embedding_matrix[i] = model.wv[model.wv.index2word[i]]\n",
    "    return embedding_matrix, model.wv.index2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__γ)__ Θα επιλέξουμε 10 τυχαίες λέξεις από το λεξικό μας και θα βρούμε τις σημασιολογικά κοντινότερές τους."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "selected_words = random.sample(voc, 10)\n",
    "for word in selected_words:\n",
    "    # get most similar words\n",
    "    sim = model.wv.most_similar(word, topn=5)\n",
    "    print('\"' + word + '\"' + \" is similar with the following words:\")\n",
    "    for s in sim:\n",
    "        print('\"' + s[0] + '\"' + \" with similarity \" + str(s[1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Θα επιλέξουμε τώρα μεγαλύτερο παράθυρο κρατώντας ίδιο τον αριθμό των εποχών."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize word2vec. Context is taken as the 2 previous and 2 next words\n",
    "model = Word2Vec(tokenized_sentences, window=20, size=100, workers=4)\n",
    "# Train the model for 1000 epochs\n",
    "model.train(tokenized_sentences, total_examples=len(sentences), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ordered vocabulary list\n",
    "voc = model.wv.index2word\n",
    "# get vector size\n",
    "dim = model.vector_size\n",
    "\n",
    "import random\n",
    "\n",
    "selected_words = random.sample(voc, 10)\n",
    "for word in selected_words:\n",
    "    # get most similar words\n",
    "    sim = model.wv.most_similar(word, topn=5)\n",
    "    print('\"' + word + '\"' + \" is similar with the following words:\")\n",
    "    for s in sim:\n",
    "        print('\"' + s[0] + '\"' + \" with similarity \" + str(s[1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αντίθετα τώρα θα αυξήσουμε τον αριθμό των εποχών κρατώντας το μέγεθος του παραθύρου σταθερό."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize word2vec. Context is taken as the 2 previous and 2 next words\n",
    "model = Word2Vec(tokenized_sentences, window=5, size=100, workers=4)\n",
    "# Train the model for 1000 epochs\n",
    "model.train(tokenized_sentences, total_examples=len(sentences), epochs=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ordered vocabulary list\n",
    "voc = model.wv.index2word\n",
    "# get vector size\n",
    "dim = model.vector_size\n",
    "\n",
    "import random\n",
    "\n",
    "selected_words = random.sample(voc, 10)\n",
    "for word in selected_words:\n",
    "    # get most similar words\n",
    "    sim = model.wv.most_similar(word, topn=5)\n",
    "    print('\"' + word + '\"' + \" is similar with the following words:\")\n",
    "    for s in sim:\n",
    "        print('\"' + s[0] + '\"' + \" with similarity \" + str(s[1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Σχολιασμός Αποτελεσμάτων"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τα αποτελέσματα δεν είναι τόσο ποιοτικά όσο περιμέναμε. Παρατηρούμε ότι σε κάποιες περιπτώσεις πετυχαίνει 1-2 λέξεις που όντως είναι σημασιολογικά κοντά με την επιλεγόμενη λέξη. Ακόμα και όταν αυξήσαμε το μέγεθος του παραθύρου ή τον αριθμό των εποχών η κατάσταση παρέμεινε η ίδια.Κατά την γνώμη μου αυτό συμβαίνει διότι έχουμε ένα πολύ μικρό corpus που δεν βοηθάει το μοντέλο ώστε να καταλάβει ποιες λέξεις είναι πραγματικά κοντά σημασιολογικά."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
